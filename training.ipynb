{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder, KBinsDiscretizer, PolynomialFeatures, FunctionTransformer, TargetEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, RFE, RFECV, SelectFromModel, mutual_info_regression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, VotingClassifier, VotingRegressor, AdaBoostClassifier, RandomForestClassifier, HistGradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from typing import List, Tuple, Any, Dict, Literal\n",
    "from scipy.stats import uniform, randint\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\clean_data.csv', encoding='utf-8') as f:\n",
    "    data = pd.read_csv(f)\n",
    "# display full columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando target encoder a todas las variables categoricas excluyendo la varivle FECHA_ACCIDENTE\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "for column in categorical_columns:\n",
    "    if column != 'FECHA_ACCIDENTE' and column != 'GRAVEDAD_ACCIDENTE':\n",
    "        target_encoder = TargetEncoder()\n",
    "        data[column] = target_encoder.fit_transform(data[column].values.reshape(-1,1), data['GRAVEDAD_ACCIDENTE'])\n",
    "# transformar la variable objetivo a binario donde 1 es grave y 0 es leve\n",
    "data['GRAVEDAD_ACCIDENTE'] = data['GRAVEDAD_ACCIDENTE'].apply(lambda x: 1 if x == 'ACCIDENTE INCAPACITANTE' else 0)\n",
    "# RobustScaler para las variables numericas\n",
    "scaler = RobustScaler()\n",
    "data[['EDAD']] = scaler.fit_transform(data[['EDAD']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRAVEDAD_ACCIDENTE\n",
       "0    9667\n",
       "1    2310\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('GRAVEDAD_ACCIDENTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\clean_data_target_encoder.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construir un modelo de clasificaci√≥n  XGBoost primero sin seleccion de variables\n",
    "X_1 = data.drop(['GRAVEDAD_ACCIDENTE','FECHA_CORTE','PERIODO_REGISTRO','FECHA_ACCIDENTE','DIAS_DESCANZO','MONTO_DESCANSO'], axis=1)\n",
    "y_1 = data['GRAVEDAD_ACCIDENTE']\n",
    "from models.classifier_model import TreeEnsemblePipeline, Pipeline\n",
    "clf_pipeline = TreeEnsemblePipeline()\n",
    "model, X_test, y_test = clf_pipeline.build_model(X_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 12:13:25,979 - INFO - Starting model building process with 11977 samples...\n",
      "2024-11-03 12:13:25,989 - INFO - Original data statistics:\n",
      "2024-11-03 12:13:25,990 - INFO - Original statistics:\n",
      "2024-11-03 12:13:25,992 - INFO - Mean: 4.46\n",
      "2024-11-03 12:13:25,993 - INFO - Std: 6.92\n",
      "2024-11-03 12:13:25,996 - INFO - Min: 0.00\n",
      "2024-11-03 12:13:25,996 - INFO - Max: 180.00\n",
      "2024-11-03 12:13:26,001 - INFO - Quartiles:\n",
      "0.25    1.0\n",
      "0.50    3.0\n",
      "0.75    5.0\n",
      "Name: DIAS_DESCANZO, dtype: float64\n",
      "2024-11-03 12:13:26,004 - INFO - Number of potential outliers: 638\n",
      "2024-11-03 12:13:26,007 - INFO - \n",
      "Winsorized data statistics:\n",
      "2024-11-03 12:13:26,007 - INFO - Winsorized statistics:\n",
      "2024-11-03 12:13:26,009 - INFO - Mean: 3.29\n",
      "2024-11-03 12:13:26,011 - INFO - Std: 2.52\n",
      "2024-11-03 12:13:26,011 - INFO - Min: 0.00\n",
      "2024-11-03 12:13:26,011 - INFO - Max: 8.00\n",
      "2024-11-03 12:13:26,016 - INFO - Quartiles:\n",
      "0.25    1.0\n",
      "0.50    3.0\n",
      "0.75    5.0\n",
      "dtype: float64\n",
      "2024-11-03 12:13:26,019 - INFO - Number of potential outliers: 0\n",
      "2024-11-03 12:13:26,032 - INFO - Fitting model...\n",
      "2024-11-03 12:16:38,155 - INFO - Best RMSE score: 2.3419\n",
      "2024-11-03 12:16:38,155 - INFO - Best parameters: {'model__gbm__base_model__learning_rate': 0.10636199770892528, 'model__gbm__base_model__max_depth': 3, 'model__gbm__base_model__n_estimators': 165, 'model__gbm__base_model__subsample': 0.7883346676208757, 'model__histgb__base_model__l2_regularization': 1.2701954572038505, 'model__histgb__base_model__learning_rate': 0.09511366715168569, 'model__histgb__base_model__max_depth': 6, 'model__rf__base_model__max_depth': 7, 'model__rf__base_model__min_samples_leaf': 4, 'model__rf__base_model__n_estimators': 67, 'model__xgb__base_model__colsample_bytree': 0.867040378737505, 'model__xgb__base_model__learning_rate': 0.1036154774160781, 'model__xgb__base_model__max_depth': 4, 'model__xgb__base_model__n_estimators': 116, 'model__xgb__base_model__subsample': 0.8710183510268095}\n"
     ]
    }
   ],
   "source": [
    "# Definir las variables para entrenar el modelo de regresion\n",
    "X_2 = data.drop(['GRAVEDAD_ACCIDENTE','FECHA_CORTE','PERIODO_REGISTRO','FECHA_ACCIDENTE','DIAS_DESCANZO','MONTO_DESCANSO'], axis=1)\n",
    "y_2 = data['DIAS_DESCANZO']\n",
    "\n",
    "from models.model_pipeline import OptimizedEnsemblePipeline, ModelConfig\n",
    "\n",
    "# Configura y ejecuta el modelo\n",
    "config = ModelConfig(winsor_limits=(0.10, 0.90))\n",
    "days_pipeline = OptimizedEnsemblePipeline(config)\n",
    "model, X_test, y_test, training_rmse = days_pipeline.build_model(X_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 12:16:48,720 - INFO - Starting model building process with 11977 samples...\n",
      "2024-11-03 12:16:48,730 - INFO - Original data statistics:\n",
      "2024-11-03 12:16:48,730 - INFO - Original statistics:\n",
      "2024-11-03 12:16:48,733 - INFO - Mean: 314.20\n",
      "2024-11-03 12:16:48,734 - INFO - Std: 825.92\n",
      "2024-11-03 12:16:48,734 - INFO - Min: 0.00\n",
      "2024-11-03 12:16:48,737 - INFO - Max: 42227.42\n",
      "2024-11-03 12:16:48,740 - INFO - Quartiles:\n",
      "0.25     65.81\n",
      "0.50    151.24\n",
      "0.75    297.16\n",
      "Name: MONTO_DESCANSO, dtype: float64\n",
      "2024-11-03 12:16:48,743 - INFO - Number of potential outliers: 917\n",
      "2024-11-03 12:16:48,746 - INFO - \n",
      "Winsorized data statistics:\n",
      "2024-11-03 12:16:48,746 - INFO - Winsorized statistics:\n",
      "2024-11-03 12:16:48,748 - INFO - Mean: 207.37\n",
      "2024-11-03 12:16:48,751 - INFO - Std: 191.03\n",
      "2024-11-03 12:16:48,752 - INFO - Min: 0.00\n",
      "2024-11-03 12:16:48,754 - INFO - Max: 613.87\n",
      "2024-11-03 12:16:48,755 - INFO - Quartiles:\n",
      "0.25     65.81\n",
      "0.50    151.24\n",
      "0.75    297.16\n",
      "dtype: float64\n",
      "2024-11-03 12:16:48,761 - INFO - Number of potential outliers: 0\n",
      "2024-11-03 12:16:48,779 - INFO - Fitting model...\n",
      "2024-11-03 12:20:36,951 - INFO - Best RMSE score: 177.4239\n",
      "2024-11-03 12:20:36,951 - INFO - Best parameters: {'model__gbm__base_model__learning_rate': 0.0814595104179952, 'model__gbm__base_model__max_depth': 5, 'model__gbm__base_model__n_estimators': 194, 'model__gbm__base_model__subsample': 0.8196462704334383, 'model__histgb__base_model__l2_regularization': 1.367041475161484, 'model__histgb__base_model__learning_rate': 0.08440426429991153, 'model__histgb__base_model__max_depth': 4, 'model__rf__base_model__max_depth': 9, 'model__rf__base_model__min_samples_leaf': 2, 'model__rf__base_model__n_estimators': 166, 'model__xgb__base_model__colsample_bytree': 0.7242618899851593, 'model__xgb__base_model__learning_rate': 0.05283144749401078, 'model__xgb__base_model__max_depth': 6, 'model__xgb__base_model__n_estimators': 178, 'model__xgb__base_model__subsample': 0.7519882960212537}\n",
      "2024-11-03 12:20:36,961 - ERROR - Error in model evaluation: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m model, X_test, y_test, training_rmse \u001b[38;5;241m=\u001b[39m amount_pipeline\u001b[38;5;241m.\u001b[39mbuild_model(X_3, y_3)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mamount_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\models\\model_pipeline.py:285\u001b[0m, in \u001b[0;36mOptimizedEnsemblePipeline.evaluate_model\u001b[1;34m(self, model, X_test, y_test)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the model on test data with proper winsorization\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# Apply winsorization to test features using limits learned from training\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     X_test_winsorized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_winsorizer_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_winsorized)\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# For fair evaluation, also winsorize test targets using limits learned from training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\models\\model_pipeline.py:59\u001b[0m, in \u001b[0;36mWinsorizer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m     58\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (lower, upper) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_bounds_\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     60\u001b[0m         X\u001b[38;5;241m.\u001b[39miloc[:, i] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mclip(lower\u001b[38;5;241m=\u001b[39mlower, upper\u001b[38;5;241m=\u001b[39mupper)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Definir las variables para entrenar el modelo de regresion para MONTO_DESCANSO\n",
    "X_3 = data.drop(['GRAVEDAD_ACCIDENTE','FECHA_CORTE','PERIODO_REGISTRO','FECHA_ACCIDENTE','DIAS_DESCANZO','MONTO_DESCANSO'], axis=1)\n",
    "y_3 = data['MONTO_DESCANSO']\n",
    "from models.model_pipeline import OptimizedEnsemblePipeline, ModelConfig\n",
    "# Crear la configuraci√≥n\n",
    "config = ModelConfig(\n",
    "    n_features=12,\n",
    "    test_size=0.2,\n",
    "    cv_folds=3,\n",
    "    winsor_limits=(0.10, 0.90)\n",
    ")\n",
    "\n",
    "# Inicializar la pipeline\n",
    "amount_pipeline = OptimizedEnsemblePipeline(config)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model, X_test, y_test, training_rmse = amount_pipeline.build_model(X_3, y_3)\n",
    "\n",
    "# Evaluar el modelo\n",
    "metrics = amount_pipeline.evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TreeEnsemblePipeline' object has no attribute 'X_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelos y caracter√≠sticas guardados en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Uso en Jupyter Notebook:\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43msave_trained_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclf_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclf_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tu pipeline de clasificaci√≥n\u001b[39;49;00m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdays_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdays_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tu pipeline de d√≠as\u001b[39;49;00m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamount_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamount_pipeline\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tu pipeline de monto\u001b[39;49;00m\n\u001b[0;32m     48\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 24\u001b[0m, in \u001b[0;36msave_trained_models\u001b[1;34m(clf_pipeline, days_pipeline, amount_pipeline, output_dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m Path(output_dir)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Asegurarse de que los selectores de caracter√≠sticas est√©n ajustados\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m clf_pipeline\u001b[38;5;241m.\u001b[39mfeature_selector\u001b[38;5;241m.\u001b[39mfit(\u001b[43mclf_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m, clf_pipeline\u001b[38;5;241m.\u001b[39my_train)\n\u001b[0;32m     25\u001b[0m days_pipeline\u001b[38;5;241m.\u001b[39mfeature_selector\u001b[38;5;241m.\u001b[39mfit(days_pipeline\u001b[38;5;241m.\u001b[39mX_train, days_pipeline\u001b[38;5;241m.\u001b[39my_train)\n\u001b[0;32m     26\u001b[0m amount_pipeline\u001b[38;5;241m.\u001b[39mfeature_selector\u001b[38;5;241m.\u001b[39mfit(amount_pipeline\u001b[38;5;241m.\u001b[39mX_train, amount_pipeline\u001b[38;5;241m.\u001b[39my_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TreeEnsemblePipeline' object has no attribute 'X_train'"
     ]
    }
   ],
   "source": [
    "# Script 1: save_models.py (para usar en Jupyter Notebook)\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_trained_models(clf_pipeline, days_pipeline, amount_pipeline, output_dir=r'C:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\trained_models'):\n",
    "    \"\"\"\n",
    "    Guarda los modelos entrenados y sus caracter√≠sticas seleccionadas.\n",
    "    \n",
    "    Par√°metros:\n",
    "    -----------\n",
    "    clf_pipeline : TreeEnsemblePipeline\n",
    "        Pipeline entrenado para clasificaci√≥n de gravedad\n",
    "    days_pipeline : OptimizedEnsemblePipeline\n",
    "        Pipeline entrenado para predicci√≥n de d√≠as\n",
    "    amount_pipeline : OptimizedEnsemblePipeline\n",
    "        Pipeline entrenado para predicci√≥n de monto\n",
    "    output_dir : str\n",
    "        Directorio donde se guardar√°n los modelos\n",
    "    \"\"\"\n",
    "    # Crear directorio si no existe\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Asegurarse de que los selectores de caracter√≠sticas est√©n ajustados\n",
    "    clf_pipeline.feature_selector.fit(clf_pipeline.X_train, clf_pipeline.y_train)\n",
    "    days_pipeline.feature_selector.fit(days_pipeline.X_train, days_pipeline.y_train)\n",
    "    amount_pipeline.feature_selector.fit(amount_pipeline.X_train, amount_pipeline.y_train)\n",
    "    \n",
    "    # Guardar modelos\n",
    "    joblib.dump(clf_pipeline, f'{output_dir}/clf_pipeline.joblib')\n",
    "    joblib.dump(days_pipeline, f'{output_dir}/days_pipeline.joblib')\n",
    "    joblib.dump(amount_pipeline, f'{output_dir}/amount_pipeline.joblib')\n",
    "    \n",
    "    # Guardar caracter√≠sticas seleccionadas\n",
    "    features = {\n",
    "        'clf': clf_pipeline.feature_selector.get_support(),\n",
    "        'days': days_pipeline.feature_selector.get_support(),\n",
    "        'amount': amount_pipeline.feature_selector.get_support()\n",
    "    }\n",
    "    joblib.dump(features, f'{output_dir}/selected_features.joblib')\n",
    "    \n",
    "    print(f\"Modelos y caracter√≠sticas guardados en {output_dir}/\")\n",
    "\n",
    "# Uso en Jupyter Notebook:\n",
    "save_trained_models(\n",
    "    clf_pipeline=clf_pipeline,  # Tu pipeline de clasificaci√≥n\n",
    "    days_pipeline=days_pipeline,  # Tu pipeline de d√≠as\n",
    "    amount_pipeline=amount_pipeline  # Tu pipeline de monto\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
