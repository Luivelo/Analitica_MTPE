{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder, KBinsDiscretizer, PolynomialFeatures, FunctionTransformer, TargetEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, RFE, RFECV, SelectFromModel, mutual_info_regression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, VotingClassifier, VotingRegressor, AdaBoostClassifier, RandomForestClassifier, HistGradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from typing import List, Tuple, Any, Dict, Literal\n",
    "from scipy.stats import uniform, randint\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\clean_data.csv', encoding='utf-8') as f:\n",
    "    data = pd.read_csv(f)\n",
    "# display full columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando target encoder a todas las variables categoricas excluyendo la varivle FECHA_ACCIDENTE\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "for column in categorical_columns:\n",
    "    if column != 'FECHA_ACCIDENTE' and column != 'GRAVEDAD_ACCIDENTE':\n",
    "        target_encoder = TargetEncoder()\n",
    "        data[column] = target_encoder.fit_transform(data[column].values.reshape(-1,1), data['GRAVEDAD_ACCIDENTE'])\n",
    "# transformar la variable objetivo a binario donde 1 es grave y 0 es leve\n",
    "data['GRAVEDAD_ACCIDENTE'] = data['GRAVEDAD_ACCIDENTE'].apply(lambda x: 1 if x == 'ACCIDENTE INCAPACITANTE' else 0)\n",
    "# RobustScaler para las variables numericas\n",
    "scaler = RobustScaler()\n",
    "data[['EDAD']] = scaler.fit_transform(data[['EDAD']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRAVEDAD_ACCIDENTE\n",
       "0    9667\n",
       "1    2310\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('GRAVEDAD_ACCIDENTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\clean_data_target_encoder.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construir un modelo de clasificación  XGBoost primero sin seleccion de variables\n",
    "X_1 = data.drop(['GRAVEDAD_ACCIDENTE','FECHA_CORTE','PERIODO_REGISTRO','FECHA_ACCIDENTE','DIAS_DESCANZO','MONTO_DESCANSO'], axis=1)\n",
    "y_1 = data['GRAVEDAD_ACCIDENTE']\n",
    "from models.classifier_model import TreeEnsemblePipeline, Pipeline\n",
    "clf_pipeline = TreeEnsemblePipeline()\n",
    "model, X_test, y_test = clf_pipeline.build_model(X_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 12:13:25,979 - INFO - Starting model building process with 11977 samples...\n",
      "2024-11-03 12:13:25,989 - INFO - Original data statistics:\n",
      "2024-11-03 12:13:25,990 - INFO - Original statistics:\n",
      "2024-11-03 12:13:25,992 - INFO - Mean: 4.46\n",
      "2024-11-03 12:13:25,993 - INFO - Std: 6.92\n",
      "2024-11-03 12:13:25,996 - INFO - Min: 0.00\n",
      "2024-11-03 12:13:25,996 - INFO - Max: 180.00\n",
      "2024-11-03 12:13:26,001 - INFO - Quartiles:\n",
      "0.25    1.0\n",
      "0.50    3.0\n",
      "0.75    5.0\n",
      "Name: DIAS_DESCANZO, dtype: float64\n",
      "2024-11-03 12:13:26,004 - INFO - Number of potential outliers: 638\n",
      "2024-11-03 12:13:26,007 - INFO - \n",
      "Winsorized data statistics:\n",
      "2024-11-03 12:13:26,007 - INFO - Winsorized statistics:\n",
      "2024-11-03 12:13:26,009 - INFO - Mean: 3.29\n",
      "2024-11-03 12:13:26,011 - INFO - Std: 2.52\n",
      "2024-11-03 12:13:26,011 - INFO - Min: 0.00\n",
      "2024-11-03 12:13:26,011 - INFO - Max: 8.00\n",
      "2024-11-03 12:13:26,016 - INFO - Quartiles:\n",
      "0.25    1.0\n",
      "0.50    3.0\n",
      "0.75    5.0\n",
      "dtype: float64\n",
      "2024-11-03 12:13:26,019 - INFO - Number of potential outliers: 0\n",
      "2024-11-03 12:13:26,032 - INFO - Fitting model...\n",
      "2024-11-03 12:16:38,155 - INFO - Best RMSE score: 2.3419\n",
      "2024-11-03 12:16:38,155 - INFO - Best parameters: {'model__gbm__base_model__learning_rate': 0.10636199770892528, 'model__gbm__base_model__max_depth': 3, 'model__gbm__base_model__n_estimators': 165, 'model__gbm__base_model__subsample': 0.7883346676208757, 'model__histgb__base_model__l2_regularization': 1.2701954572038505, 'model__histgb__base_model__learning_rate': 0.09511366715168569, 'model__histgb__base_model__max_depth': 6, 'model__rf__base_model__max_depth': 7, 'model__rf__base_model__min_samples_leaf': 4, 'model__rf__base_model__n_estimators': 67, 'model__xgb__base_model__colsample_bytree': 0.867040378737505, 'model__xgb__base_model__learning_rate': 0.1036154774160781, 'model__xgb__base_model__max_depth': 4, 'model__xgb__base_model__n_estimators': 116, 'model__xgb__base_model__subsample': 0.8710183510268095}\n"
     ]
    }
   ],
   "source": [
    "# Definir las variables para entrenar el modelo de regresion\n",
    "X_2 = data.drop(['GRAVEDAD_ACCIDENTE','FECHA_CORTE','PERIODO_REGISTRO','FECHA_ACCIDENTE','DIAS_DESCANZO','MONTO_DESCANSO'], axis=1)\n",
    "y_2 = data['DIAS_DESCANZO']\n",
    "\n",
    "from models.model_pipeline import OptimizedEnsemblePipeline, ModelConfig\n",
    "\n",
    "# Configura y ejecuta el modelo\n",
    "config = ModelConfig(winsor_limits=(0.10, 0.90))\n",
    "days_pipeline = OptimizedEnsemblePipeline(config)\n",
    "model, X_test, y_test, training_rmse = days_pipeline.build_model(X_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 12:16:48,720 - INFO - Starting model building process with 11977 samples...\n",
      "2024-11-03 12:16:48,730 - INFO - Original data statistics:\n",
      "2024-11-03 12:16:48,730 - INFO - Original statistics:\n",
      "2024-11-03 12:16:48,733 - INFO - Mean: 314.20\n",
      "2024-11-03 12:16:48,734 - INFO - Std: 825.92\n",
      "2024-11-03 12:16:48,734 - INFO - Min: 0.00\n",
      "2024-11-03 12:16:48,737 - INFO - Max: 42227.42\n",
      "2024-11-03 12:16:48,740 - INFO - Quartiles:\n",
      "0.25     65.81\n",
      "0.50    151.24\n",
      "0.75    297.16\n",
      "Name: MONTO_DESCANSO, dtype: float64\n",
      "2024-11-03 12:16:48,743 - INFO - Number of potential outliers: 917\n",
      "2024-11-03 12:16:48,746 - INFO - \n",
      "Winsorized data statistics:\n",
      "2024-11-03 12:16:48,746 - INFO - Winsorized statistics:\n",
      "2024-11-03 12:16:48,748 - INFO - Mean: 207.37\n",
      "2024-11-03 12:16:48,751 - INFO - Std: 191.03\n",
      "2024-11-03 12:16:48,752 - INFO - Min: 0.00\n",
      "2024-11-03 12:16:48,754 - INFO - Max: 613.87\n",
      "2024-11-03 12:16:48,755 - INFO - Quartiles:\n",
      "0.25     65.81\n",
      "0.50    151.24\n",
      "0.75    297.16\n",
      "dtype: float64\n",
      "2024-11-03 12:16:48,761 - INFO - Number of potential outliers: 0\n",
      "2024-11-03 12:16:48,779 - INFO - Fitting model...\n",
      "2024-11-03 12:20:36,951 - INFO - Best RMSE score: 177.4239\n",
      "2024-11-03 12:20:36,951 - INFO - Best parameters: {'model__gbm__base_model__learning_rate': 0.0814595104179952, 'model__gbm__base_model__max_depth': 5, 'model__gbm__base_model__n_estimators': 194, 'model__gbm__base_model__subsample': 0.8196462704334383, 'model__histgb__base_model__l2_regularization': 1.367041475161484, 'model__histgb__base_model__learning_rate': 0.08440426429991153, 'model__histgb__base_model__max_depth': 4, 'model__rf__base_model__max_depth': 9, 'model__rf__base_model__min_samples_leaf': 2, 'model__rf__base_model__n_estimators': 166, 'model__xgb__base_model__colsample_bytree': 0.7242618899851593, 'model__xgb__base_model__learning_rate': 0.05283144749401078, 'model__xgb__base_model__max_depth': 6, 'model__xgb__base_model__n_estimators': 178, 'model__xgb__base_model__subsample': 0.7519882960212537}\n",
      "2024-11-03 12:20:36,961 - ERROR - Error in model evaluation: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m model, X_test, y_test, training_rmse \u001b[38;5;241m=\u001b[39m amount_pipeline\u001b[38;5;241m.\u001b[39mbuild_model(X_3, y_3)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mamount_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\models\\model_pipeline.py:285\u001b[0m, in \u001b[0;36mOptimizedEnsemblePipeline.evaluate_model\u001b[1;34m(self, model, X_test, y_test)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the model on test data with proper winsorization\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# Apply winsorization to test features using limits learned from training\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     X_test_winsorized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_winsorizer_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_winsorized)\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# For fair evaluation, also winsorize test targets using limits learned from training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\models\\model_pipeline.py:59\u001b[0m, in \u001b[0;36mWinsorizer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m     58\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (lower, upper) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_bounds_\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     60\u001b[0m         X\u001b[38;5;241m.\u001b[39miloc[:, i] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mclip(lower\u001b[38;5;241m=\u001b[39mlower, upper\u001b[38;5;241m=\u001b[39mupper)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Definir las variables para entrenar el modelo de regresion para MONTO_DESCANSO\n",
    "X_3 = data.drop(['GRAVEDAD_ACCIDENTE','FECHA_CORTE','PERIODO_REGISTRO','FECHA_ACCIDENTE','DIAS_DESCANZO','MONTO_DESCANSO'], axis=1)\n",
    "y_3 = data['MONTO_DESCANSO']\n",
    "from models.model_pipeline import OptimizedEnsemblePipeline, ModelConfig\n",
    "# Crear la configuración\n",
    "config = ModelConfig(\n",
    "    n_features=12,\n",
    "    test_size=0.2,\n",
    "    cv_folds=3,\n",
    "    winsor_limits=(0.10, 0.90)\n",
    ")\n",
    "\n",
    "# Inicializar la pipeline\n",
    "amount_pipeline = OptimizedEnsemblePipeline(config)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model, X_test, y_test, training_rmse = amount_pipeline.build_model(X_3, y_3)\n",
    "\n",
    "# Evaluar el modelo\n",
    "metrics = amount_pipeline.evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TreeEnsemblePipeline' object has no attribute 'X_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelos y características guardados en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Uso en Jupyter Notebook:\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43msave_trained_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclf_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclf_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tu pipeline de clasificación\u001b[39;49;00m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdays_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdays_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tu pipeline de días\u001b[39;49;00m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamount_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamount_pipeline\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tu pipeline de monto\u001b[39;49;00m\n\u001b[0;32m     48\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 24\u001b[0m, in \u001b[0;36msave_trained_models\u001b[1;34m(clf_pipeline, days_pipeline, amount_pipeline, output_dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m Path(output_dir)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Asegurarse de que los selectores de características estén ajustados\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m clf_pipeline\u001b[38;5;241m.\u001b[39mfeature_selector\u001b[38;5;241m.\u001b[39mfit(\u001b[43mclf_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m, clf_pipeline\u001b[38;5;241m.\u001b[39my_train)\n\u001b[0;32m     25\u001b[0m days_pipeline\u001b[38;5;241m.\u001b[39mfeature_selector\u001b[38;5;241m.\u001b[39mfit(days_pipeline\u001b[38;5;241m.\u001b[39mX_train, days_pipeline\u001b[38;5;241m.\u001b[39my_train)\n\u001b[0;32m     26\u001b[0m amount_pipeline\u001b[38;5;241m.\u001b[39mfeature_selector\u001b[38;5;241m.\u001b[39mfit(amount_pipeline\u001b[38;5;241m.\u001b[39mX_train, amount_pipeline\u001b[38;5;241m.\u001b[39my_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TreeEnsemblePipeline' object has no attribute 'X_train'"
     ]
    }
   ],
   "source": [
    "# Script 1: save_models.py (para usar en Jupyter Notebook)\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_trained_models(clf_pipeline, days_pipeline, amount_pipeline, output_dir=r'C:\\Users\\USER\\Desktop\\MTPE\\Analitica_MTPE\\trained_models'):\n",
    "    \"\"\"\n",
    "    Guarda los modelos entrenados y sus características seleccionadas.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    clf_pipeline : TreeEnsemblePipeline\n",
    "        Pipeline entrenado para clasificación de gravedad\n",
    "    days_pipeline : OptimizedEnsemblePipeline\n",
    "        Pipeline entrenado para predicción de días\n",
    "    amount_pipeline : OptimizedEnsemblePipeline\n",
    "        Pipeline entrenado para predicción de monto\n",
    "    output_dir : str\n",
    "        Directorio donde se guardarán los modelos\n",
    "    \"\"\"\n",
    "    # Crear directorio si no existe\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Asegurarse de que los selectores de características estén ajustados\n",
    "    clf_pipeline.feature_selector.fit(clf_pipeline.X_train, clf_pipeline.y_train)\n",
    "    days_pipeline.feature_selector.fit(days_pipeline.X_train, days_pipeline.y_train)\n",
    "    amount_pipeline.feature_selector.fit(amount_pipeline.X_train, amount_pipeline.y_train)\n",
    "    \n",
    "    # Guardar modelos\n",
    "    joblib.dump(clf_pipeline, f'{output_dir}/clf_pipeline.joblib')\n",
    "    joblib.dump(days_pipeline, f'{output_dir}/days_pipeline.joblib')\n",
    "    joblib.dump(amount_pipeline, f'{output_dir}/amount_pipeline.joblib')\n",
    "    \n",
    "    # Guardar características seleccionadas\n",
    "    features = {\n",
    "        'clf': clf_pipeline.feature_selector.get_support(),\n",
    "        'days': days_pipeline.feature_selector.get_support(),\n",
    "        'amount': amount_pipeline.feature_selector.get_support()\n",
    "    }\n",
    "    joblib.dump(features, f'{output_dir}/selected_features.joblib')\n",
    "    \n",
    "    print(f\"Modelos y características guardados en {output_dir}/\")\n",
    "\n",
    "# Uso en Jupyter Notebook:\n",
    "save_trained_models(\n",
    "    clf_pipeline=clf_pipeline,  # Tu pipeline de clasificación\n",
    "    days_pipeline=days_pipeline,  # Tu pipeline de días\n",
    "    amount_pipeline=amount_pipeline  # Tu pipeline de monto\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
